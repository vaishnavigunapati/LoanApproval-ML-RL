{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5220c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Relig\\AppData\\Local\\Temp\\ipykernel_19968\\1477745971.py:13: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"C:\\Users\\Relig\\Downloads\\LoanApproval-ML-RL\\data\\accepted_2007_to_2018.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([1013425, 7])  Test shape: torch.Size([253357, 7])\n",
      "Epoch 0, Loss: 0.694806694984436\n",
      "Epoch 2, Loss: 0.6453614830970764\n",
      "Epoch 4, Loss: 0.6059209108352661\n",
      "Epoch 6, Loss: 0.5737894773483276\n",
      "Epoch 8, Loss: 0.5474066734313965\n",
      "AUC: 0.5919672934965796\n",
      "F1-Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# ------------------------\n",
    "# 1. Load & Preprocess Data\n",
    "# ------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Relig\\Downloads\\LoanApproval-ML-RL\\data\\accepted_2007_to_2018.csv\")\n",
    "\n",
    "# Simplify target\n",
    "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]\n",
    "df['loan_status'] = df['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "\n",
    "# Feature selection\n",
    "features = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', \n",
    "            'emp_length', 'home_ownership', 'purpose']\n",
    "df = df[features + ['loan_status']].dropna()\n",
    "\n",
    "# Encode categorical\n",
    "for col in ['home_ownership','purpose','emp_length']:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Scale numerical\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# Train/test split\n",
    "X = df[features]\n",
    "y = df['loan_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).view(-1,1)\n",
    "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.values, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "print(\"Train shape:\", X_train_t.shape, \" Test shape:\", X_test_t.shape)\n",
    "\n",
    "# ------------------------\n",
    "# 2. Define Model\n",
    "# ------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = MLP(X_train.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ------------------------\n",
    "# 3. Training Loop\n",
    "# ------------------------\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_t)\n",
    "    loss = criterion(y_pred, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. Evaluation\n",
    "# ------------------------\n",
    "y_test_pred = model(X_test_t).detach().numpy()\n",
    "auc = roc_auc_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, (y_test_pred>0.5).astype(int))\n",
    "\n",
    "print(\"AUC:\", auc)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: torch.Size([1013425, 7])  Test shape: torch.Size([253357, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1013425, 1] doesn't match the broadcast shape [1013425, 1013425]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     99\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X_train_t)\n\u001b[1;32m--> 100\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Relig\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Relig\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Relig\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:699\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Relig\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3569\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3566\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3567\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1013425, 1] doesn't match the broadcast shape [1013425, 1013425]"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 1_EDA_Preprocessing_and_DL_Fixed.ipynb\n",
    "# =============================\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ IMPORT LIBRARIES\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, RocCurveDisplay\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ LOAD & PREPROCESS DATA\n",
    "# -----------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\Relig\\Downloads\\LoanApproval-ML-RL\\data\\accepted_2007_to_2018.csv\", low_memory=False)\n",
    "\n",
    "# Keep only relevant target classes\n",
    "df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "df['loan_status'] = df['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "\n",
    "# Select features\n",
    "features = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'emp_length', 'home_ownership', 'purpose']\n",
    "df = df[features + ['loan_status']].dropna()\n",
    "\n",
    "# Convert interest rate to float\n",
    "df['int_rate'] = df['int_rate'].astype(str).str.rstrip('%')\n",
    "df['int_rate'] = pd.to_numeric(df['int_rate'], errors='coerce') / 100.0\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical features\n",
    "for col in ['home_ownership','purpose','emp_length']:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# Train/test split with stratification\n",
    "X = df[features]\n",
    "y = df['loan_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).view(-1,1)\n",
    "X_test_t = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test.values, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "print(\"Train shape:\", X_train_t.shape, \" Test shape:\", X_test_t.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ DEFINE MLP MODEL (NO SIGMOID)\n",
    "# -----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)  # no sigmoid\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = MLP(X_train.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ HANDLE CLASS IMBALANCE\n",
    "# -----------------------------\n",
    "num_pos = y_train.sum()\n",
    "num_neg = len(y_train) - num_pos\n",
    "pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32)  # for BCEWithLogitsLoss\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ TRAINING LOOP\n",
    "# -----------------------------\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_t)\n",
    "    loss = criterion(y_pred, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ EVALUATION\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "y_test_pred_logits = model(X_test_t).detach()\n",
    "y_test_pred = torch.sigmoid(y_test_pred_logits).numpy()  # apply sigmoid here\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, (y_test_pred>0.5).astype(int))\n",
    "print(\"\\n===== EVALUATION =====\")\n",
    "print(\"AUC:\", round(auc,4))\n",
    "print(\"F1-Score:\", round(f1,4))\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred_label = (y_test_pred>0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred_label)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# ROC Curve\n",
    "RocCurveDisplay.from_predictions(y_test, y_test_pred)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ NOTES / JUSTIFICATIONS\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "Model Justification:\n",
    "- MLP with 2 hidden layers (128 → 64 neurons) captures non-linear feature interactions.\n",
    "- ReLU activations for faster convergence.\n",
    "- Dropout (0.2) reduces overfitting.\n",
    "- No Sigmoid in final layer: use BCEWithLogitsLoss with pos_weight to handle imbalance.\n",
    "- Class imbalance handled using pos_weight (~neg/pos ratio).\n",
    "\n",
    "Next Steps:\n",
    "- Can tune learning rate, batch size, or layers.\n",
    "- Could apply SMOTE or advanced architectures.\n",
    "- Metrics ready for comparison with RL agent in Task 4.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
